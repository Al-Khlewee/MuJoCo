{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3192f0d",
   "metadata": {},
   "source": [
    "# Unitree G1 Humanoid Training on Google Colab\n",
    "\n",
    "This notebook trains a PPO policy for the Unitree G1 humanoid robot using Brax and MuJoCo MJX.\n",
    "\n",
    "**Based on the official [MuJoCo MJX Tutorial](https://colab.research.google.com/github/google-deepmind/mujoco/blob/main/mjx/tutorial.ipynb)**\n",
    "\n",
    "## Setup Instructions:\n",
    "1. **Runtime ‚Üí Change runtime type ‚Üí T4 GPU** (or A100 for faster training)\n",
    "2. Run all cells in order\n",
    "3. Training takes ~10-15 minutes on A100 GPU, ~60-90 minutes on T4 GPU\n",
    "4. Download trained policy from Files panel\n",
    "\n",
    "## Features:\n",
    "- ‚úÖ Humanoid locomotion (forward walking)\n",
    "- ‚úÖ Domain randomization (friction + actuator parameters)\n",
    "- ‚úÖ PPO training with Brax\n",
    "- ‚úÖ 29 DOF (6 per leg, 3 waist, 7 per arm)\n",
    "- ‚úÖ Policy visualization and video export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daee5e55",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4b3de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mujoco mujoco_mjx brax\n",
    "!pip install flax optax orbax-checkpoint\n",
    "!pip install mediapy\n",
    "!command -v ffmpeg >/dev/null || (apt update && apt install -y ffmpeg)\n",
    "\n",
    "print(\"\\n‚úÖ All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b5f4a6",
   "metadata": {},
   "source": [
    "## 2. Verify GPU and Configure Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f85b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Check GPU and Configure Environment\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Check GPU\n",
    "if subprocess.run('nvidia-smi').returncode:\n",
    "  raise RuntimeError(\n",
    "      'Cannot communicate with GPU. '\n",
    "      'Make sure you are using a GPU Colab runtime. '\n",
    "      'Go to the Runtime menu and select Choose runtime type.')\n",
    "\n",
    "# Add Nvidia EGL driver config for rendering\n",
    "NVIDIA_ICD_CONFIG_PATH = '/usr/share/glvnd/egl_vendor.d/10_nvidia.json'\n",
    "if not os.path.exists(NVIDIA_ICD_CONFIG_PATH):\n",
    "  with open(NVIDIA_ICD_CONFIG_PATH, 'w') as f:\n",
    "    f.write(\"\"\"{{\n",
    "    \"file_format_version\" : \"1.0.0\",\n",
    "    \"ICD\" : {{\n",
    "        \"library_path\" : \"libEGL_nvidia.so.0\"\n",
    "    }}\n",
    "}}\n",
    "\"\"\")\n",
    "\n",
    "# Configure MuJoCo to use EGL rendering (GPU)\n",
    "os.environ['MUJOCO_GL'] = 'egl'\n",
    "\n",
    "# XLA optimization for better performance\n",
    "xla_flags = os.environ.get('XLA_FLAGS', '')\n",
    "xla_flags += ' --xla_gpu_triton_gemm_any=True'\n",
    "os.environ['XLA_FLAGS'] = xla_flags\n",
    "\n",
    "# Test MuJoCo installation\n",
    "import mujoco\n",
    "mujoco.MjModel.from_xml_string('<mujoco/>')\n",
    "\n",
    "import jax\n",
    "print(f\"‚úÖ GPU detected: {jax.devices()}\")\n",
    "print(f\"‚úÖ MuJoCo configured with EGL rendering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3827c1",
   "metadata": {},
   "source": [
    "## 3. Download MuJoCo Menagerie Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd98e6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/google-deepmind/mujoco_menagerie.git\n",
    "print(\"\\n‚úÖ MuJoCo Menagerie downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef90b4b",
   "metadata": {},
   "source": [
    "## 4. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2173b1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from typing import Any, Dict, List, Sequence, Tuple\n",
    "from datetime import datetime\n",
    "from etils import epath\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mediapy as media\n",
    "from IPython.display import HTML\n",
    "\n",
    "from ml_collections import config_dict\n",
    "from flax.training import orbax_utils\n",
    "from orbax import checkpoint as ocp\n",
    "\n",
    "import mujoco\n",
    "from mujoco import mjx\n",
    "\n",
    "from brax import base\n",
    "from brax import envs\n",
    "from brax import math\n",
    "from brax.base import Motion, Transform\n",
    "from brax.envs.base import PipelineEnv, State\n",
    "from brax.training.agents.ppo import train as ppo\n",
    "from brax.training.agents.ppo import networks as ppo_networks\n",
    "from brax.io import html, mjcf, model\n",
    "\n",
    "# More legible numpy printing\n",
    "np.set_printoptions(precision=3, suppress=True, linewidth=100)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1aa6902",
   "metadata": {},
   "source": [
    "## 5. Define Unitree G1 Humanoid Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756e3343",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Unitree G1 Humanoid Environment Definition (Based on MJX Tutorial Humanoid)\n",
    "\n",
    "G1_ROOT_PATH = epath.Path('mujoco_menagerie/unitree_g1')\n",
    "\n",
    "\n",
    "class UnitreeG1Env(PipelineEnv):\n",
    "  \"\"\"Environment for training the Unitree G1 humanoid in MJX.\"\"\"\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      forward_reward_weight=1.25,\n",
    "      ctrl_cost_weight=0.1,\n",
    "      healthy_reward=5.0,\n",
    "      terminate_when_unhealthy=True,\n",
    "      healthy_z_range=(0.6, 2.0),  # G1 is taller than Humanoid\n",
    "      reset_noise_scale=1e-2,\n",
    "      exclude_current_positions_from_observation=True,\n",
    "      **kwargs,\n",
    "  ):\n",
    "    path = G1_ROOT_PATH / 'scene_mjx.xml'\n",
    "    mj_model = mujoco.MjModel.from_xml_path(path.as_posix())\n",
    "    \n",
    "    # Optimize solver for G1\n",
    "    mj_model.opt.solver = mujoco.mjtSolver.mjSOL_CG\n",
    "    mj_model.opt.iterations = 6\n",
    "    mj_model.opt.ls_iterations = 6\n",
    "\n",
    "    sys = mjcf.load_model(mj_model)\n",
    "\n",
    "    physics_steps_per_control_step = 5\n",
    "    kwargs['n_frames'] = kwargs.get(\n",
    "        'n_frames', physics_steps_per_control_step)\n",
    "    kwargs['backend'] = 'mjx'\n",
    "\n",
    "    super().__init__(sys, **kwargs)\n",
    "\n",
    "    self._forward_reward_weight = forward_reward_weight\n",
    "    self._ctrl_cost_weight = ctrl_cost_weight\n",
    "    self._healthy_reward = healthy_reward\n",
    "    self._terminate_when_unhealthy = terminate_when_unhealthy\n",
    "    self._healthy_z_range = healthy_z_range\n",
    "    self._reset_noise_scale = reset_noise_scale\n",
    "    self._exclude_current_positions_from_observation = (\n",
    "        exclude_current_positions_from_observation\n",
    "    )\n",
    "\n",
    "  def reset(self, rng: jp.ndarray) -> State:\n",
    "    \"\"\"Resets the environment to an initial state.\"\"\"\n",
    "    rng, rng1, rng2 = jax.random.split(rng, 3)\n",
    "\n",
    "    low, hi = -self._reset_noise_scale, self._reset_noise_scale\n",
    "    qpos = self.sys.qpos0 + jax.random.uniform(\n",
    "        rng1, (self.sys.nq,), minval=low, maxval=hi\n",
    "    )\n",
    "    qvel = jax.random.uniform(\n",
    "        rng2, (self.sys.nv,), minval=low, maxval=hi\n",
    "    )\n",
    "\n",
    "    data = self.pipeline_init(qpos, qvel)\n",
    "\n",
    "    obs = self._get_obs(data, jp.zeros(self.sys.nu))\n",
    "    reward, done, zero = jp.zeros(3)\n",
    "    metrics = {\n",
    "        'forward_reward': zero,\n",
    "        'reward_linvel': zero,\n",
    "        'reward_quadctrl': zero,\n",
    "        'reward_alive': zero,\n",
    "        'x_position': zero,\n",
    "        'y_position': zero,\n",
    "        'distance_from_origin': zero,\n",
    "        'x_velocity': zero,\n",
    "        'y_velocity': zero,\n",
    "    }\n",
    "    return State(data, obs, reward, done, metrics)\n",
    "\n",
    "  def step(self, state: State, action: jp.ndarray) -> State:\n",
    "    \"\"\"Runs one timestep of the environment's dynamics.\"\"\"\n",
    "    data0 = state.pipeline_state\n",
    "    data = self.pipeline_step(data0, action)\n",
    "\n",
    "    # Center of mass velocity (for G1, pelvis is body index 1)\n",
    "    com_before = data0.subtree_com[1]\n",
    "    com_after = data.subtree_com[1]\n",
    "    velocity = (com_after - com_before) / self.dt\n",
    "    forward_reward = self._forward_reward_weight * velocity[0]\n",
    "\n",
    "    min_z, max_z = self._healthy_z_range\n",
    "    is_healthy = jp.where(data.q[2] < min_z, 0.0, 1.0)\n",
    "    is_healthy = jp.where(data.q[2] > max_z, 0.0, is_healthy)\n",
    "    if self._terminate_when_unhealthy:\n",
    "      healthy_reward = self._healthy_reward\n",
    "    else:\n",
    "      healthy_reward = self._healthy_reward * is_healthy\n",
    "\n",
    "    ctrl_cost = self._ctrl_cost_weight * jp.sum(jp.square(action))\n",
    "\n",
    "    obs = self._get_obs(data, action)\n",
    "    reward = forward_reward + healthy_reward - ctrl_cost\n",
    "    done = 1.0 - is_healthy if self._terminate_when_unhealthy else 0.0\n",
    "    state.metrics.update(\n",
    "        forward_reward=forward_reward,\n",
    "        reward_linvel=forward_reward,\n",
    "        reward_quadctrl=-ctrl_cost,\n",
    "        reward_alive=healthy_reward,\n",
    "        x_position=com_after[0],\n",
    "        y_position=com_after[1],\n",
    "        distance_from_origin=jp.linalg.norm(com_after),\n",
    "        x_velocity=velocity[0],\n",
    "        y_velocity=velocity[1],\n",
    "    )\n",
    "\n",
    "    return state.replace(\n",
    "        pipeline_state=data, obs=obs, reward=reward, done=done\n",
    "    )\n",
    "\n",
    "  def _get_obs(\n",
    "      self, data: mjx.Data, action: jp.ndarray\n",
    "  ) -> jp.ndarray:\n",
    "    \"\"\"Observes G1 body position, velocities, and angles.\"\"\"\n",
    "    position = data.qpos\n",
    "    if self._exclude_current_positions_from_observation:\n",
    "      position = position[2:]\n",
    "\n",
    "    # External contact forces are excluded\n",
    "    return jp.concatenate([\n",
    "        position,\n",
    "        data.qvel,\n",
    "        data.cinert[1:].ravel(),\n",
    "        data.cvel[1:].ravel(),\n",
    "        data.qfrc_actuator,\n",
    "    ])\n",
    "\n",
    "\n",
    "envs.register_environment('unitree_g1', UnitreeG1Env)\n",
    "print(\"‚úÖ UnitreeG1Env registered!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19074114",
   "metadata": {},
   "source": [
    "## 6. Domain Randomization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccc7aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Domain Randomization Function\n",
    "\n",
    "def domain_randomize(sys, rng):\n",
    "  \"\"\"Randomizes the mjx.Model for better sim-to-real transfer.\"\"\"\n",
    "  @jax.vmap\n",
    "  def rand(rng):\n",
    "    _, key = jax.random.split(rng, 2)\n",
    "    # friction randomization\n",
    "    friction = jax.random.uniform(key, (1,), minval=0.6, maxval=1.4)\n",
    "    friction = sys.geom_friction.at[:, 0].set(friction)\n",
    "    # actuator randomization\n",
    "    _, key = jax.random.split(key, 2)\n",
    "    gain_range = (-5, 5)\n",
    "    param = jax.random.uniform(\n",
    "        key, (1,), minval=gain_range[0], maxval=gain_range[1]\n",
    "    ) + sys.actuator_gainprm[:, 0]\n",
    "    gain = sys.actuator_gainprm.at[:, 0].set(param)\n",
    "    bias = sys.actuator_biasprm.at[:, 1].set(-param)\n",
    "    return friction, gain, bias\n",
    "\n",
    "  friction, gain, bias = rand(rng)\n",
    "\n",
    "  in_axes = jax.tree_util.tree_map(lambda x: None, sys)\n",
    "  in_axes = in_axes.tree_replace({\n",
    "      'geom_friction': 0,\n",
    "      'actuator_gainprm': 0,\n",
    "      'actuator_biasprm': 0,\n",
    "  })\n",
    "\n",
    "  sys = sys.tree_replace({\n",
    "      'geom_friction': friction,\n",
    "      'actuator_gainprm': gain,\n",
    "      'actuator_biasprm': bias,\n",
    "  })\n",
    "\n",
    "  return sys, in_axes\n",
    "\n",
    "\n",
    "print(\"‚úÖ Domain randomization function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25825a9",
   "metadata": {},
   "source": [
    "## 7. Create Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f6ad8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env_name = 'unitree_g1'\n",
    "env = envs.get_environment(env_name)\n",
    "\n",
    "print(f\"‚úÖ Environment created!\")\n",
    "print(f\"  - Observation size: {env.observation_size}\")\n",
    "print(f\"  - Action size: {env.action_size}\")\n",
    "print(f\"  - DOF: {env.sys.nq} position, {env.sys.nv} velocity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3582ac98",
   "metadata": {},
   "source": [
    "## 8. Visualize Untrained Policy (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe163093",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Quick Test: Visualize Random Actions\n",
    "\n",
    "jit_reset = jax.jit(env.reset)\n",
    "jit_step = jax.jit(env.step)\n",
    "\n",
    "# Initialize\n",
    "state = jit_reset(jax.random.PRNGKey(0))\n",
    "rollout = [state.pipeline_state]\n",
    "\n",
    "# Grab a short trajectory with random actions\n",
    "for i in range(50):\n",
    "  ctrl = 0.1 * jp.ones(env.sys.nu)  # Small random action\n",
    "  state = jit_step(state, ctrl)\n",
    "  rollout.append(state.pipeline_state)\n",
    "\n",
    "media.show_video(env.render(rollout, camera='side'), fps=1.0 / env.dt)\n",
    "print(\"\\n‚ö†Ô∏è This is WITHOUT training - the robot will likely fall!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dfd41a",
   "metadata": {},
   "source": [
    "## 9. Train Policy with Progress Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1d7143",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Train Policy with Progress Tracking\n",
    "\n",
    "# Training configuration (humanoids need more steps than quadrupeds)\n",
    "NUM_TIMESTEPS = 20_000_000  # 20M steps for humanoid (vs 100M for very good performance)\n",
    "NUM_EVALS = 5\n",
    "EPISODE_LENGTH = 1000\n",
    "NUM_ENVS = 3072  # Slightly fewer envs due to larger state space\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "# Progress tracking\n",
    "x_data = []\n",
    "y_data = []\n",
    "ydataerr = []\n",
    "times = [datetime.now()]\n",
    "max_y, min_y = 13000, 0\n",
    "\n",
    "def progress(num_steps, metrics):\n",
    "  \"\"\"Callback to track and display training progress.\"\"\"\n",
    "  times.append(datetime.now())\n",
    "  x_data.append(num_steps)\n",
    "  y_data.append(metrics['eval/episode_reward'])\n",
    "  ydataerr.append(metrics['eval/episode_reward_std'])\n",
    "\n",
    "  plt.xlim([0, NUM_TIMESTEPS * 1.25])\n",
    "  plt.ylim([min_y, max_y])\n",
    "  plt.xlabel('# environment steps')\n",
    "  plt.ylabel('reward per episode')\n",
    "  plt.title(f'Unitree G1 Training | Reward: {y_data[-1]:.3f}')\n",
    "  plt.errorbar(x_data, y_data, yerr=ydataerr)\n",
    "  plt.grid(True, alpha=0.3)\n",
    "  plt.show()\n",
    "\n",
    "  print(f\"Steps: {num_steps:,} | Reward: {metrics['eval/episode_reward']:.2f}\")\n",
    "\n",
    "# Setup checkpoint saving\n",
    "ckpt_path = epath.Path('/tmp/unitree_g1_humanoid/ckpts')\n",
    "ckpt_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def policy_params_fn(current_step, make_policy, params):\n",
    "  \"\"\"Save checkpoints during training.\"\"\"\n",
    "  orbax_checkpointer = ocp.PyTreeCheckpointer()\n",
    "  save_args = orbax_utils.save_args_from_target(params)\n",
    "  path = ckpt_path / f'{current_step}'\n",
    "  orbax_checkpointer.save(path, params, force=True, save_args=save_args)\n",
    "  print(f\"üíæ Checkpoint saved: {path}\")\n",
    "\n",
    "# Configure PPO training (matching tutorial's Humanoid settings)\n",
    "make_networks_factory = functools.partial(\n",
    "    ppo_networks.make_ppo_networks,\n",
    "    policy_hidden_layer_sizes=(256, 256, 256))  # Larger network for humanoid\n",
    "\n",
    "train_fn = functools.partial(\n",
    "    ppo.train,\n",
    "    num_timesteps=NUM_TIMESTEPS,\n",
    "    num_evals=NUM_EVALS,\n",
    "    reward_scaling=0.1,\n",
    "    episode_length=EPISODE_LENGTH,\n",
    "    normalize_observations=True,\n",
    "    action_repeat=1,\n",
    "    unroll_length=10,\n",
    "    num_minibatches=24,\n",
    "    num_updates_per_batch=8,\n",
    "    discounting=0.97,\n",
    "    learning_rate=3e-4,\n",
    "    entropy_cost=1e-3,\n",
    "    num_envs=NUM_ENVS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    network_factory=make_networks_factory,\n",
    "    randomization_fn=domain_randomize,\n",
    "    policy_params_fn=policy_params_fn,\n",
    "    seed=0,\n",
    "    progress_fn=progress\n",
    ")\n",
    "\n",
    "# Reset environment and start training\n",
    "env = envs.get_environment(env_name)\n",
    "eval_env = envs.get_environment(env_name)\n",
    "\n",
    "print(f\"\\nüöÄ Starting training...\")\n",
    "print(f\"  - Total steps: {NUM_TIMESTEPS:,}\")\n",
    "print(f\"  - Parallel envs: {NUM_ENVS}\")\n",
    "print(f\"  - Estimated time: ~10-15 min on A100, ~60-90 min on T4 GPU\")\n",
    "print(f\"  - Note: Humanoids are harder to train than quadrupeds!\\n\")\n",
    "\n",
    "make_inference_fn, params, _ = train_fn(\n",
    "    environment=env,\n",
    "    eval_env=eval_env\n",
    ")\n",
    "\n",
    "print(f'\\nüéâ Training completed!')\n",
    "print(f'  - Time to JIT: {times[1] - times[0]}')\n",
    "print(f'  - Time to train: {times[-1] - times[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af703279",
   "metadata": {},
   "source": [
    "## 10. Plot Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd01debb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x_data, y_data, 'b-', linewidth=2)\n",
    "plt.fill_between(x_data, \n",
    "                 [y - err for y, err in zip(y_data, ydataerr)],\n",
    "                 [y + err for y, err in zip(y_data, ydataerr)],\n",
    "                 alpha=0.3)\n",
    "plt.xlabel('Training Steps', fontsize=12)\n",
    "plt.ylabel('Episode Reward', fontsize=12)\n",
    "plt.title('Unitree G1 Humanoid Training Progress', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd03fc6",
   "metadata": {},
   "source": [
    "## 11. Save Trained Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b7c8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and reload params\n",
    "model_path = '/tmp/mjx_brax_unitree_g1_policy'\n",
    "model.save_params(model_path, params)\n",
    "\n",
    "print(f\"‚úÖ Final policy saved: {model_path}\")\n",
    "print(f\"\\nüì¶ To download:\")\n",
    "print(f\"1. Click Files icon (üìÅ) in left sidebar\")\n",
    "print(f\"2. Navigate to {ckpt_path} or {model_path}\")\n",
    "print(f\"3. Right-click ‚Üí Download\")\n",
    "\n",
    "# Reload params and create inference function\n",
    "params = model.load_params(model_path)\n",
    "inference_fn = make_inference_fn(params)\n",
    "jit_inference_fn = jax.jit(inference_fn)\n",
    "\n",
    "print(f\"\\n‚úÖ Policy reloaded and ready for visualization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3900aa1c",
   "metadata": {},
   "source": [
    "## 12. Visualize Trained Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a6b58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Visualize Trained Policy\n",
    "\n",
    "# Create evaluation environment\n",
    "eval_env = envs.get_environment(env_name)\n",
    "jit_reset = jax.jit(eval_env.reset)\n",
    "jit_step = jax.jit(eval_env.step)\n",
    "\n",
    "# Initialize\n",
    "rng = jax.random.PRNGKey(1)\n",
    "state = jit_reset(rng)\n",
    "rollout = [state.pipeline_state]\n",
    "\n",
    "# Run trajectory\n",
    "n_steps = 500\n",
    "render_every = 2\n",
    "\n",
    "print(f\"Running {n_steps} steps with trained policy...\")\n",
    "for i in range(n_steps):\n",
    "  act_rng, rng = jax.random.split(rng)\n",
    "  ctrl, _ = jit_inference_fn(state.obs, act_rng)\n",
    "  state = jit_step(state, ctrl)\n",
    "  rollout.append(state.pipeline_state)\n",
    "  \n",
    "  if state.done:\n",
    "    print(f\"Episode terminated at step {i}\")\n",
    "    break\n",
    "\n",
    "print(f\"‚úÖ Simulation completed: {len(rollout)} frames\")\n",
    "\n",
    "# Render with MuJoCo renderer\n",
    "media.show_video(\n",
    "    eval_env.render(rollout[::render_every], camera='side'),\n",
    "    fps=1.0 / eval_env.dt / render_every)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c44b58d",
   "metadata": {},
   "source": [
    "## 13. Alternative: Render with Brax HTML Renderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0946d3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Alternative: Render with Brax HTML Renderer\n",
    "\n",
    "# This provides an interactive 3D visualization\n",
    "HTML(html.render(\n",
    "    eval_env.sys.tree_replace({'opt.timestep': eval_env.dt}),\n",
    "    rollout\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb11145",
   "metadata": {},
   "source": [
    "## 14. Package and Download All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0187a119",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Package and Download All Results\n",
    "\n",
    "# Create zip file with all outputs\n",
    "!zip -r /tmp/unitree_g1_training_results.zip {ckpt_path} {model_path}\n",
    "\n",
    "print(\"\\nüì¶ All results packaged: /tmp/unitree_g1_training_results.zip\")\n",
    "print(\"\\n‚úÖ Download from Files panel (üìÅ)\")\n",
    "print(\"\\nContains:\")\n",
    "print(f\"  - Final policy: {model_path}\")\n",
    "print(f\"  - Checkpoints: {ckpt_path}\")\n",
    "print(f\"\\nüí° Use these files with a local inference script!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e718f47",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What was trained:\n",
    "- **Robot**: Unitree G1 humanoid from [MuJoCo Menagerie](https://github.com/google-deepmind/mujoco_menagerie/tree/main/unitree_g1)\n",
    "- **Task**: Forward walking locomotion\n",
    "- **Algorithm**: PPO (Proximal Policy Optimization)\n",
    "- **Training steps**: 20M (20 million environment steps)\n",
    "- **Parallel environments**: 3072 (GPU-accelerated with MJX)\n",
    "- **Domain randomization**: Friction (0.6-1.4x) + Actuator parameters (¬±5 gain)\n",
    "\n",
    "### Robot Specifications:\n",
    "- **DOF**: 29 actuated joints\n",
    "  - Legs: 6 DOF each (hip pitch/roll/yaw, knee, ankle pitch/roll)\n",
    "  - Waist: 3 DOF (yaw, roll, pitch)\n",
    "  - Arms: 7 DOF each (shoulder pitch/roll/yaw, elbow, wrist roll/pitch/yaw)\n",
    "- **Height**: ~1.3m standing\n",
    "- **Observation space**: ~400 dimensions (includes position, velocity, inertia, forces)\n",
    "\n",
    "### Files generated:\n",
    "1. **`/tmp/mjx_brax_unitree_g1_policy`** - Final trained policy\n",
    "2. **`/tmp/unitree_g1_humanoid/ckpts/`** - Intermediate checkpoints\n",
    "3. **Training progress plot** - Reward vs. steps visualization\n",
    "4. **Video demos** - Rendered humanoid locomotion\n",
    "\n",
    "### Training Notes:\n",
    "- Humanoid locomotion is **much harder** than quadruped locomotion\n",
    "- 20M steps gives basic walking; 100M+ steps for robust locomotion\n",
    "- The G1 has a **complex state space** with 29 actuators\n",
    "- Domain randomization helps with sim-to-real transfer\n",
    "- Based on the [MuJoCo Playground](https://playground.mujoco.org/) project\n",
    "\n",
    "### Expected Behavior:\n",
    "With 20M steps training:\n",
    "- ‚úÖ Basic forward walking\n",
    "- ‚úÖ Maintaining balance\n",
    "- ‚ö†Ô∏è May fall occasionally\n",
    "\n",
    "For production-quality policies, train for 100M+ steps (~30-60 min on A100).\n",
    "\n",
    "### Based on:\n",
    "- [Official MuJoCo MJX Tutorial](https://colab.research.google.com/github/google-deepmind/mujoco/blob/main/mjx/tutorial.ipynb)\n",
    "- [MuJoCo Playground Paper](https://arxiv.org/abs/2502.08844)\n",
    "- Unitree G1 model from MuJoCo Menagerie"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
