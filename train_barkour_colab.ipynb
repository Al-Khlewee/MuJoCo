{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c9f7a74",
   "metadata": {},
   "source": [
    "# Barkour Quadruped Training on Google Colab\n",
    "\n",
    "This notebook trains a PPO policy for the Google Barkour VB quadruped robot using Brax and MuJoCo MJX.\n",
    "\n",
    "**Based on the official [MuJoCo MJX Tutorial](https://colab.research.google.com/github/google-deepmind/mujoco/blob/main/mjx/tutorial.ipynb)**\n",
    "\n",
    "## Setup Instructions:\n",
    "1. **Runtime ‚Üí Change runtime type ‚Üí T4 GPU** (or A100 for faster training)\n",
    "2. Run all cells in order\n",
    "3. Training takes ~6 minutes on A100 GPU, ~30-45 minutes on T4 GPU\n",
    "4. Download trained policy from Files panel\n",
    "\n",
    "## Features:\n",
    "- ‚úÖ Joystick control (x_vel, y_vel, ang_vel commands)\n",
    "- ‚úÖ Domain randomization (friction + actuator parameters)\n",
    "- ‚úÖ PPO training with Brax\n",
    "- ‚úÖ Policy visualization and video export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e098eba",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5917cda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mujoco mujoco_mjx brax\n",
    "!pip install flax optax orbax-checkpoint\n",
    "!pip install mediapy\n",
    "!command -v ffmpeg >/dev/null || (apt update && apt install -y ffmpeg)\n",
    "\n",
    "print(\"\\n‚úÖ All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90659e4d",
   "metadata": {},
   "source": [
    "## 2. Verify GPU is Available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b98561f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Check GPU and Configure Environment\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Check GPU\n",
    "if subprocess.run('nvidia-smi').returncode:\n",
    "  raise RuntimeError(\n",
    "      'Cannot communicate with GPU. '\n",
    "      'Make sure you are using a GPU Colab runtime. '\n",
    "      'Go to the Runtime menu and select Choose runtime type.')\n",
    "\n",
    "# Add Nvidia EGL driver config for rendering\n",
    "NVIDIA_ICD_CONFIG_PATH = '/usr/share/glvnd/egl_vendor.d/10_nvidia.json'\n",
    "if not os.path.exists(NVIDIA_ICD_CONFIG_PATH):\n",
    "  with open(NVIDIA_ICD_CONFIG_PATH, 'w') as f:\n",
    "    f.write(\"\"\"{\n",
    "    \"file_format_version\" : \"1.0.0\",\n",
    "    \"ICD\" : {\n",
    "        \"library_path\" : \"libEGL_nvidia.so.0\"\n",
    "    }\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "# Configure MuJoCo to use EGL rendering (GPU)\n",
    "os.environ['MUJOCO_GL'] = 'egl'\n",
    "\n",
    "# XLA optimization for better performance\n",
    "xla_flags = os.environ.get('XLA_FLAGS', '')\n",
    "xla_flags += ' --xla_gpu_triton_gemm_any=True'\n",
    "os.environ['XLA_FLAGS'] = xla_flags\n",
    "\n",
    "# Test MuJoCo installation\n",
    "import mujoco\n",
    "mujoco.MjModel.from_xml_string('<mujoco/>')\n",
    "\n",
    "import jax\n",
    "print(f\"‚úÖ GPU detected: {jax.devices()}\")\n",
    "print(f\"‚úÖ MuJoCo configured with EGL rendering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be7779e",
   "metadata": {},
   "source": [
    "## 3. Download MuJoCo Menagerie Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaf8f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/google-deepmind/mujoco_menagerie.git\n",
    "print(\"\\n‚úÖ MuJoCo Menagerie downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad162ef",
   "metadata": {},
   "source": [
    "## 4. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cce520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from typing import Any, Dict, List, Sequence, Tuple\n",
    "from datetime import datetime\n",
    "from etils import epath\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mediapy as media\n",
    "from IPython.display import HTML\n",
    "\n",
    "from ml_collections import config_dict\n",
    "from flax.training import orbax_utils\n",
    "from orbax import checkpoint as ocp\n",
    "\n",
    "import mujoco\n",
    "from mujoco import mjx\n",
    "\n",
    "from brax import base\n",
    "from brax import envs\n",
    "from brax import math\n",
    "from brax.base import Motion, Transform\n",
    "from brax.envs.base import PipelineEnv, State\n",
    "from brax.training.agents.ppo import train as ppo\n",
    "from brax.training.agents.ppo import networks as ppo_networks\n",
    "from brax.io import html, mjcf, model\n",
    "\n",
    "# More legible numpy printing\n",
    "np.set_printoptions(precision=3, suppress=True, linewidth=100)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba13051",
   "metadata": {},
   "source": [
    "## 5. Define Barkour Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45edd6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Barkour Environment Definition (from official MJX tutorial)\n",
    "\n",
    "BARKOUR_ROOT_PATH = epath.Path('mujoco_menagerie/google_barkour_vb')\n",
    "\n",
    "\n",
    "def get_config():\n",
    "  \"\"\"Returns reward config for barkour quadruped environment.\"\"\"\n",
    "\n",
    "  def get_default_rewards_config():\n",
    "    default_config = config_dict.ConfigDict(\n",
    "        dict(\n",
    "            scales=config_dict.ConfigDict(\n",
    "                dict(\n",
    "                    tracking_lin_vel=1.5,\n",
    "                    tracking_ang_vel=0.8,\n",
    "                    lin_vel_z=-2.0,\n",
    "                    ang_vel_xy=-0.05,\n",
    "                    orientation=-5.0,\n",
    "                    torques=-0.0002,\n",
    "                    action_rate=-0.01,\n",
    "                    feet_air_time=0.2,\n",
    "                    stand_still=-0.5,\n",
    "                    termination=-1.0,\n",
    "                    foot_slip=-0.1,\n",
    "                )\n",
    "            ),\n",
    "            tracking_sigma=0.25,\n",
    "        )\n",
    "    )\n",
    "    return default_config\n",
    "\n",
    "  default_config = config_dict.ConfigDict(\n",
    "      dict(\n",
    "          rewards=get_default_rewards_config(),\n",
    "      )\n",
    "  )\n",
    "\n",
    "  return default_config\n",
    "\n",
    "\n",
    "class BarkourEnv(PipelineEnv):\n",
    "  \"\"\"Environment for training the barkour quadruped joystick policy in MJX.\"\"\"\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      obs_noise: float = 0.05,\n",
    "      action_scale: float = 0.3,\n",
    "      kick_vel: float = 0.05,\n",
    "      scene_file: str = 'scene_mjx.xml',\n",
    "      **kwargs,\n",
    "  ):\n",
    "    path = BARKOUR_ROOT_PATH / scene_file\n",
    "    sys = mjcf.load(path.as_posix())\n",
    "    self._dt = 0.02  # this environment is 50 fps\n",
    "    sys = sys.tree_replace({'opt.timestep': 0.004})\n",
    "\n",
    "    # override menagerie params for smoother policy\n",
    "    sys = sys.replace(\n",
    "        dof_damping=sys.dof_damping.at[6:].set(0.5239),\n",
    "        actuator_gainprm=sys.actuator_gainprm.at[:, 0].set(35.0),\n",
    "        actuator_biasprm=sys.actuator_biasprm.at[:, 1].set(-35.0),\n",
    "    )\n",
    "\n",
    "    n_frames = kwargs.pop('n_frames', int(self._dt / sys.opt.timestep))\n",
    "    super().__init__(sys, backend='mjx', n_frames=n_frames)\n",
    "\n",
    "    self.reward_config = get_config()\n",
    "    # set custom from kwargs\n",
    "    for k, v in kwargs.items():\n",
    "      if k.endswith('_scale'):\n",
    "        self.reward_config.rewards.scales[k[:-6]] = v\n",
    "\n",
    "    self._torso_idx = mujoco.mj_name2id(\n",
    "        sys.mj_model, mujoco.mjtObj.mjOBJ_BODY.value, 'torso'\n",
    "    )\n",
    "    self._action_scale = action_scale\n",
    "    self._obs_noise = obs_noise\n",
    "    self._kick_vel = kick_vel\n",
    "    self._init_q = jp.array(sys.mj_model.keyframe('home').qpos)\n",
    "    self._default_pose = sys.mj_model.keyframe('home').qpos[7:]\n",
    "    self.lowers = jp.array([-0.7, -1.0, 0.05] * 4)\n",
    "    self.uppers = jp.array([0.52, 2.1, 2.1] * 4)\n",
    "    feet_site = [\n",
    "        'foot_front_left',\n",
    "        'foot_hind_left',\n",
    "        'foot_front_right',\n",
    "        'foot_hind_right',\n",
    "    ]\n",
    "    feet_site_id = [\n",
    "        mujoco.mj_name2id(sys.mj_model, mujoco.mjtObj.mjOBJ_SITE.value, f)\n",
    "        for f in feet_site\n",
    "    ]\n",
    "    assert not any(id_ == -1 for id_ in feet_site_id), 'Site not found.'\n",
    "    self._feet_site_id = np.array(feet_site_id)\n",
    "    lower_leg_body = [\n",
    "        'lower_leg_front_left',\n",
    "        'lower_leg_hind_left',\n",
    "        'lower_leg_front_right',\n",
    "        'lower_leg_hind_right',\n",
    "    ]\n",
    "    lower_leg_body_id = [\n",
    "        mujoco.mj_name2id(sys.mj_model, mujoco.mjtObj.mjOBJ_BODY.value, l)\n",
    "        for l in lower_leg_body\n",
    "    ]\n",
    "    assert not any(id_ == -1 for id_ in lower_leg_body_id), 'Body not found.'\n",
    "    self._lower_leg_body_id = np.array(lower_leg_body_id)\n",
    "    self._foot_radius = 0.0175\n",
    "    self._nv = sys.nv\n",
    "\n",
    "  def sample_command(self, rng: jax.Array) -> jax.Array:\n",
    "    lin_vel_x = [-0.6, 1.5]  # min max [m/s]\n",
    "    lin_vel_y = [-0.8, 0.8]  # min max [m/s]\n",
    "    ang_vel_yaw = [-0.7, 0.7]  # min max [rad/s]\n",
    "\n",
    "    _, key1, key2, key3 = jax.random.split(rng, 4)\n",
    "    lin_vel_x = jax.random.uniform(\n",
    "        key1, (1,), minval=lin_vel_x[0], maxval=lin_vel_x[1]\n",
    "    )\n",
    "    lin_vel_y = jax.random.uniform(\n",
    "        key2, (1,), minval=lin_vel_y[0], maxval=lin_vel_y[1]\n",
    "    )\n",
    "    ang_vel_yaw = jax.random.uniform(\n",
    "        key3, (1,), minval=ang_vel_yaw[0], maxval=ang_vel_yaw[1]\n",
    "    )\n",
    "    new_cmd = jp.array([lin_vel_x[0], lin_vel_y[0], ang_vel_yaw[0]])\n",
    "    return new_cmd\n",
    "\n",
    "  def reset(self, rng: jax.Array) -> State:\n",
    "    rng, key = jax.random.split(rng)\n",
    "\n",
    "    pipeline_state = self.pipeline_init(self._init_q, jp.zeros(self._nv))\n",
    "\n",
    "    state_info = {\n",
    "        'rng': rng,\n",
    "        'last_act': jp.zeros(12),\n",
    "        'last_vel': jp.zeros(12),\n",
    "        'command': self.sample_command(key),\n",
    "        'last_contact': jp.zeros(4, dtype=bool),\n",
    "        'feet_air_time': jp.zeros(4),\n",
    "        'rewards': {k: 0.0 for k in self.reward_config.rewards.scales.keys()},\n",
    "        'kick': jp.array([0.0, 0.0]),\n",
    "        'step': 0,\n",
    "    }\n",
    "\n",
    "    obs_history = jp.zeros(15 * 31)  # store 15 steps of history\n",
    "    obs = self._get_obs(pipeline_state, state_info, obs_history)\n",
    "    reward, done = jp.zeros(2)\n",
    "    metrics = {'total_dist': 0.0}\n",
    "    for k in state_info['rewards']:\n",
    "      metrics[k] = state_info['rewards'][k]\n",
    "    state = State(pipeline_state, obs, reward, done, metrics, state_info)\n",
    "    return state\n",
    "\n",
    "  def step(self, state: State, action: jax.Array) -> State:\n",
    "    rng, cmd_rng, kick_noise_2 = jax.random.split(state.info['rng'], 3)\n",
    "\n",
    "    # kick\n",
    "    push_interval = 10\n",
    "    kick_theta = jax.random.uniform(kick_noise_2, maxval=2 * jp.pi)\n",
    "    kick = jp.array([jp.cos(kick_theta), jp.sin(kick_theta)])\n",
    "    kick *= jp.mod(state.info['step'], push_interval) == 0\n",
    "    qvel = state.pipeline_state.qvel\n",
    "    qvel = qvel.at[:2].set(kick * self._kick_vel + qvel[:2])\n",
    "    state = state.tree_replace({'pipeline_state.qvel': qvel})\n",
    "\n",
    "    # physics step\n",
    "    motor_targets = self._default_pose + action * self._action_scale\n",
    "    motor_targets = jp.clip(motor_targets, self.lowers, self.uppers)\n",
    "    pipeline_state = self.pipeline_step(state.pipeline_state, motor_targets)\n",
    "    x, xd = pipeline_state.x, pipeline_state.xd\n",
    "\n",
    "    # observation data\n",
    "    obs = self._get_obs(pipeline_state, state.info, state.obs)\n",
    "    joint_angles = pipeline_state.q[7:]\n",
    "    joint_vel = pipeline_state.qd[6:]\n",
    "\n",
    "    # foot contact data based on z-position\n",
    "    foot_pos = pipeline_state.site_xpos[self._feet_site_id]\n",
    "    foot_contact_z = foot_pos[:, 2] - self._foot_radius\n",
    "    contact = foot_contact_z < 1e-3  # a mm or less off the floor\n",
    "    contact_filt_mm = contact | state.info['last_contact']\n",
    "    contact_filt_cm = (foot_contact_z < 3e-2) | state.info['last_contact']\n",
    "    first_contact = (state.info['feet_air_time'] > 0) * contact_filt_mm\n",
    "    state.info['feet_air_time'] += self.dt\n",
    "\n",
    "    # done if joint limits are reached or robot is falling\n",
    "    up = jp.array([0.0, 0.0, 1.0])\n",
    "    done = jp.dot(math.rotate(up, x.rot[self._torso_idx - 1]), up) < 0\n",
    "    done |= jp.any(joint_angles < self.lowers)\n",
    "    done |= jp.any(joint_angles > self.uppers)\n",
    "    done |= pipeline_state.x.pos[self._torso_idx - 1, 2] < 0.18\n",
    "\n",
    "    # reward\n",
    "    rewards = {\n",
    "        'tracking_lin_vel': (\n",
    "            self._reward_tracking_lin_vel(state.info['command'], x, xd)\n",
    "        ),\n",
    "        'tracking_ang_vel': (\n",
    "            self._reward_tracking_ang_vel(state.info['command'], x, xd)\n",
    "        ),\n",
    "        'lin_vel_z': self._reward_lin_vel_z(xd),\n",
    "        'ang_vel_xy': self._reward_ang_vel_xy(xd),\n",
    "        'orientation': self._reward_orientation(x),\n",
    "        'torques': self._reward_torques(pipeline_state.qfrc_actuator),\n",
    "        'action_rate': self._reward_action_rate(action, state.info['last_act']),\n",
    "        'stand_still': self._reward_stand_still(\n",
    "            state.info['command'], joint_angles,\n",
    "        ),\n",
    "        'feet_air_time': self._reward_feet_air_time(\n",
    "            state.info['feet_air_time'],\n",
    "            first_contact,\n",
    "            state.info['command'],\n",
    "        ),\n",
    "        'foot_slip': self._reward_foot_slip(pipeline_state, contact_filt_cm),\n",
    "        'termination': self._reward_termination(done, state.info['step']),\n",
    "    }\n",
    "    rewards = {\n",
    "        k: v * self.reward_config.rewards.scales[k] for k, v in rewards.items()\n",
    "    }\n",
    "    reward = jp.clip(sum(rewards.values()) * self.dt, 0.0, 10000.0)\n",
    "\n",
    "    # state management\n",
    "    state.info['kick'] = kick\n",
    "    state.info['last_act'] = action\n",
    "    state.info['last_vel'] = joint_vel\n",
    "    state.info['feet_air_time'] *= ~contact_filt_mm\n",
    "    state.info['last_contact'] = contact\n",
    "    state.info['rewards'] = rewards\n",
    "    state.info['step'] += 1\n",
    "    state.info['rng'] = rng\n",
    "\n",
    "    # sample new command if more than 500 timesteps achieved\n",
    "    state.info['command'] = jp.where(\n",
    "        state.info['step'] > 500,\n",
    "        self.sample_command(cmd_rng),\n",
    "        state.info['command'],\n",
    "    )\n",
    "    # reset the step counter when done\n",
    "    state.info['step'] = jp.where(\n",
    "        done | (state.info['step'] > 500), 0, state.info['step']\n",
    "    )\n",
    "\n",
    "    # log total displacement as a proxy metric\n",
    "    state.metrics['total_dist'] = math.normalize(x.pos[self._torso_idx - 1])[1]\n",
    "    state.metrics.update(state.info['rewards'])\n",
    "\n",
    "    done = jp.float32(done)\n",
    "    state = state.replace(\n",
    "        pipeline_state=pipeline_state, obs=obs, reward=reward, done=done\n",
    "    )\n",
    "    return state\n",
    "\n",
    "  def _get_obs(\n",
    "      self,\n",
    "      pipeline_state: base.State,\n",
    "      state_info: dict[str, Any],\n",
    "      obs_history: jax.Array,\n",
    "  ) -> jax.Array:\n",
    "    inv_torso_rot = math.quat_inv(pipeline_state.x.rot[0])\n",
    "    local_rpyrate = math.rotate(pipeline_state.xd.ang[0], inv_torso_rot)\n",
    "\n",
    "    obs = jp.concatenate([\n",
    "        jp.array([local_rpyrate[2]]) * 0.25,                 # yaw rate\n",
    "        math.rotate(jp.array([0, 0, -1]), inv_torso_rot),    # projected gravity\n",
    "        state_info['command'] * jp.array([2.0, 2.0, 0.25]),  # command\n",
    "        pipeline_state.q[7:] - self._default_pose,           # motor angles\n",
    "        state_info['last_act'],                              # last action\n",
    "    ])\n",
    "\n",
    "    # clip, noise\n",
    "    obs = jp.clip(obs, -100.0, 100.0) + self._obs_noise * jax.random.uniform(\n",
    "        state_info['rng'], obs.shape, minval=-1, maxval=1\n",
    "    )\n",
    "    # stack observations through time\n",
    "    obs = jp.roll(obs_history, obs.size).at[:obs.size].set(obs)\n",
    "\n",
    "    return obs\n",
    "\n",
    "  # ------------ reward functions----------------\n",
    "  def _reward_lin_vel_z(self, xd: Motion) -> jax.Array:\n",
    "    return jp.square(xd.vel[0, 2])\n",
    "\n",
    "  def _reward_ang_vel_xy(self, xd: Motion) -> jax.Array:\n",
    "    return jp.sum(jp.square(xd.ang[0, :2]))\n",
    "\n",
    "  def _reward_orientation(self, x: Transform) -> jax.Array:\n",
    "    up = jp.array([0.0, 0.0, 1.0])\n",
    "    rot_up = math.rotate(up, x.rot[0])\n",
    "    return jp.sum(jp.square(rot_up[:2]))\n",
    "\n",
    "  def _reward_torques(self, torques: jax.Array) -> jax.Array:\n",
    "    return jp.sqrt(jp.sum(jp.square(torques))) + jp.sum(jp.abs(torques))\n",
    "\n",
    "  def _reward_action_rate(\n",
    "      self, act: jax.Array, last_act: jax.Array\n",
    "  ) -> jax.Array:\n",
    "    return jp.sum(jp.square(act - last_act))\n",
    "\n",
    "  def _reward_tracking_lin_vel(\n",
    "      self, commands: jax.Array, x: Transform, xd: Motion\n",
    "  ) -> jax.Array:\n",
    "    local_vel = math.rotate(xd.vel[0], math.quat_inv(x.rot[0]))\n",
    "    lin_vel_error = jp.sum(jp.square(commands[:2] - local_vel[:2]))\n",
    "    lin_vel_reward = jp.exp(\n",
    "        -lin_vel_error / self.reward_config.rewards.tracking_sigma\n",
    "    )\n",
    "    return lin_vel_reward\n",
    "\n",
    "  def _reward_tracking_ang_vel(\n",
    "      self, commands: jax.Array, x: Transform, xd: Motion\n",
    "  ) -> jax.Array:\n",
    "    base_ang_vel = math.rotate(xd.ang[0], math.quat_inv(x.rot[0]))\n",
    "    ang_vel_error = jp.square(commands[2] - base_ang_vel[2])\n",
    "    return jp.exp(-ang_vel_error / self.reward_config.rewards.tracking_sigma)\n",
    "\n",
    "  def _reward_feet_air_time(\n",
    "      self, air_time: jax.Array, first_contact: jax.Array, commands: jax.Array\n",
    "  ) -> jax.Array:\n",
    "    rew_air_time = jp.sum((air_time - 0.1) * first_contact)\n",
    "    rew_air_time *= (\n",
    "        math.normalize(commands[:2])[1] > 0.05\n",
    "    )  # no reward for zero command\n",
    "    return rew_air_time\n",
    "\n",
    "  def _reward_stand_still(\n",
    "      self,\n",
    "      commands: jax.Array,\n",
    "      joint_angles: jax.Array,\n",
    "  ) -> jax.Array:\n",
    "    return jp.sum(jp.abs(joint_angles - self._default_pose)) * (\n",
    "        math.normalize(commands[:2])[1] < 0.1\n",
    "    )\n",
    "\n",
    "  def _reward_foot_slip(\n",
    "      self, pipeline_state: base.State, contact_filt: jax.Array\n",
    "  ) -> jax.Array:\n",
    "    pos = pipeline_state.site_xpos[self._feet_site_id]  # feet position\n",
    "    feet_offset = pos - pipeline_state.xpos[self._lower_leg_body_id]\n",
    "    offset = base.Transform.create(pos=feet_offset)\n",
    "    foot_indices = self._lower_leg_body_id - 1  # we got rid of the world body\n",
    "    foot_vel = offset.vmap().do(pipeline_state.xd.take(foot_indices)).vel\n",
    "\n",
    "    return jp.sum(jp.square(foot_vel[:, :2]) * contact_filt.reshape((-1, 1)))\n",
    "\n",
    "  def _reward_termination(self, done: jax.Array, step: jax.Array) -> jax.Array:\n",
    "    return done & (step < 500)\n",
    "\n",
    "  def render(\n",
    "      self, trajectory: List[base.State], camera: str | None = None,\n",
    "      width: int = 240, height: int = 320,\n",
    "  ) -> Sequence[np.ndarray]:\n",
    "    camera = camera or 'track'\n",
    "    return super().render(trajectory, camera=camera, width=width, height=height)\n",
    "\n",
    "\n",
    "envs.register_environment('barkour', BarkourEnv)\n",
    "print(\"‚úÖ BarkourEnv registered!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4f7af3",
   "metadata": {},
   "source": [
    "## 6. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66c5322",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Domain Randomization Function\n",
    "\n",
    "def domain_randomize(sys, rng):\n",
    "  \"\"\"Randomizes the mjx.Model for better sim-to-real transfer.\"\"\"\n",
    "  @jax.vmap\n",
    "  def rand(rng):\n",
    "    _, key = jax.random.split(rng, 2)\n",
    "    # friction randomization\n",
    "    friction = jax.random.uniform(key, (1,), minval=0.6, maxval=1.4)\n",
    "    friction = sys.geom_friction.at[:, 0].set(friction)\n",
    "    # actuator randomization\n",
    "    _, key = jax.random.split(key, 2)\n",
    "    gain_range = (-5, 5)\n",
    "    param = jax.random.uniform(\n",
    "        key, (1,), minval=gain_range[0], maxval=gain_range[1]\n",
    "    ) + sys.actuator_gainprm[:, 0]\n",
    "    gain = sys.actuator_gainprm.at[:, 0].set(param)\n",
    "    bias = sys.actuator_biasprm.at[:, 1].set(-param)\n",
    "    return friction, gain, bias\n",
    "\n",
    "  friction, gain, bias = rand(rng)\n",
    "\n",
    "  in_axes = jax.tree_util.tree_map(lambda x: None, sys)\n",
    "  in_axes = in_axes.tree_replace({\n",
    "      'geom_friction': 0,\n",
    "      'actuator_gainprm': 0,\n",
    "      'actuator_biasprm': 0,\n",
    "  })\n",
    "\n",
    "  sys = sys.tree_replace({\n",
    "      'geom_friction': friction,\n",
    "      'actuator_gainprm': gain,\n",
    "      'actuator_biasprm': bias,\n",
    "  })\n",
    "\n",
    "  return sys, in_axes\n",
    "\n",
    "\n",
    "print(\"‚úÖ Domain randomization function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acba2a3d",
   "metadata": {},
   "source": [
    "## 7. Create Environment with Domain Randomization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680d8482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env_name = 'barkour'\n",
    "env = envs.get_environment(env_name)\n",
    "\n",
    "print(f\"‚úÖ Environment created!\")\n",
    "print(f\"  - Observation size: {env.observation_size}\")\n",
    "print(f\"  - Action size: {env.action_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8b65f9",
   "metadata": {},
   "source": [
    "## 8. Training with Progress Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bce7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Train Policy with Progress Tracking\n",
    "\n",
    "# Training configuration\n",
    "NUM_TIMESTEPS = 100_000_000  # 100M steps (official tutorial setting)\n",
    "NUM_EVALS = 10\n",
    "EPISODE_LENGTH = 1000\n",
    "NUM_ENVS = 8192  # Large batch for GPU efficiency\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# Progress tracking\n",
    "x_data = []\n",
    "y_data = []\n",
    "ydataerr = []\n",
    "times = [datetime.now()]\n",
    "max_y, min_y = 40, 0\n",
    "\n",
    "def progress(num_steps, metrics):\n",
    "  \"\"\"Callback to track and display training progress.\"\"\"\n",
    "  times.append(datetime.now())\n",
    "  x_data.append(num_steps)\n",
    "  y_data.append(metrics['eval/episode_reward'])\n",
    "  ydataerr.append(metrics['eval/episode_reward_std'])\n",
    "\n",
    "  plt.xlim([0, NUM_TIMESTEPS * 1.25])\n",
    "  plt.ylim([min_y, max_y])\n",
    "  plt.xlabel('# environment steps')\n",
    "  plt.ylabel('reward per episode')\n",
    "  plt.title(f'Barkour Training | Reward: {y_data[-1]:.3f}')\n",
    "  plt.errorbar(x_data, y_data, yerr=ydataerr)\n",
    "  plt.grid(True, alpha=0.3)\n",
    "  plt.show()\n",
    "\n",
    "  print(f\"Steps: {num_steps:,} | Reward: {metrics['eval/episode_reward']:.2f}\")\n",
    "\n",
    "# Setup checkpoint saving\n",
    "ckpt_path = epath.Path('/tmp/barkour_joystick/ckpts')\n",
    "ckpt_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def policy_params_fn(current_step, make_policy, params):\n",
    "  \"\"\"Save checkpoints during training.\"\"\"\n",
    "  orbax_checkpointer = ocp.PyTreeCheckpointer()\n",
    "  save_args = orbax_utils.save_args_from_target(params)\n",
    "  path = ckpt_path / f'{current_step}'\n",
    "  orbax_checkpointer.save(path, params, force=True, save_args=save_args)\n",
    "  print(f\"üíæ Checkpoint saved: {path}\")\n",
    "\n",
    "# Configure PPO training\n",
    "make_networks_factory = functools.partial(\n",
    "    ppo_networks.make_ppo_networks,\n",
    "    policy_hidden_layer_sizes=(128, 128, 128, 128))\n",
    "\n",
    "train_fn = functools.partial(\n",
    "    ppo.train,\n",
    "    num_timesteps=NUM_TIMESTEPS,\n",
    "    num_evals=NUM_EVALS,\n",
    "    reward_scaling=1,\n",
    "    episode_length=EPISODE_LENGTH,\n",
    "    normalize_observations=True,\n",
    "    action_repeat=1,\n",
    "    unroll_length=20,\n",
    "    num_minibatches=32,\n",
    "    num_updates_per_batch=4,\n",
    "    discounting=0.97,\n",
    "    learning_rate=3.0e-4,\n",
    "    entropy_cost=1e-2,\n",
    "    num_envs=NUM_ENVS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    network_factory=make_networks_factory,\n",
    "    randomization_fn=domain_randomize,\n",
    "    policy_params_fn=policy_params_fn,\n",
    "    seed=0,\n",
    "    progress_fn=progress\n",
    ")\n",
    "\n",
    "# Reset environment and start training\n",
    "env = envs.get_environment(env_name)\n",
    "eval_env = envs.get_environment(env_name)\n",
    "\n",
    "print(f\"\\nüöÄ Starting training...\")\n",
    "print(f\"  - Total steps: {NUM_TIMESTEPS:,}\")\n",
    "print(f\"  - Parallel envs: {NUM_ENVS}\")\n",
    "print(f\"  - Estimated time: ~6 min on A100, ~30-45 min on T4 GPU\\n\")\n",
    "\n",
    "make_inference_fn, params, _ = train_fn(\n",
    "    environment=env,\n",
    "    eval_env=eval_env\n",
    ")\n",
    "\n",
    "print(f'\\nüéâ Training completed!')\n",
    "print(f'  - Time to JIT: {times[1] - times[0]}')\n",
    "print(f'  - Time to train: {times[-1] - times[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b74e727",
   "metadata": {},
   "source": [
    "## 9. Plot Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343dd71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(training_steps, training_rewards, 'b-', linewidth=2)\n",
    "plt.xlabel('Training Steps', fontsize=12)\n",
    "plt.ylabel('Episode Reward', fontsize=12)\n",
    "plt.title('Barkour Training Progress', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "plot_path = f\"{LOG_DIR}/training_progress.png\"\n",
    "plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "print(f\"üìä Training plot saved: {plot_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bc890e",
   "metadata": {},
   "source": [
    "## 10. Save Trained Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de0d416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and reload final policy\n",
    "model_path = '/tmp/mjx_brax_quadruped_policy'\n",
    "model.save_params(model_path, params)\n",
    "\n",
    "print(f\"‚úÖ Final policy saved: {model_path}\")\n",
    "print(f\"\\nüì¶ To download:\")\n",
    "print(f\"1. Click Files icon (üìÅ) in left sidebar\")\n",
    "print(f\"2. Navigate to {ckpt_path} or {model_path}\")\n",
    "print(f\"3. Right-click ‚Üí Download\")\n",
    "\n",
    "# Reload params and create inference function\n",
    "params = model.load_params(model_path)\n",
    "inference_fn = make_inference_fn(params)\n",
    "jit_inference_fn = jax.jit(inference_fn)\n",
    "\n",
    "print(f\"\\n‚úÖ Policy reloaded and ready for visualization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0418c2b0",
   "metadata": {},
   "source": [
    "## 11. Visualize Trained Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f207bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Visualize Trained Policy\n",
    "\n",
    "# @markdown **Joystick Commands:**\n",
    "x_vel = 1.0  #@param {type: \"number\"}\n",
    "y_vel = 0.0  #@param {type: \"number\"}\n",
    "ang_vel = -0.5  #@param {type: \"number\"}\n",
    "\n",
    "the_command = jp.array([x_vel, y_vel, ang_vel])\n",
    "\n",
    "# Create evaluation environment\n",
    "eval_env = envs.get_environment(env_name)\n",
    "jit_reset = jax.jit(eval_env.reset)\n",
    "jit_step = jax.jit(eval_env.step)\n",
    "\n",
    "# Initialize and set command\n",
    "rng = jax.random.PRNGKey(0)\n",
    "state = jit_reset(rng)\n",
    "state.info['command'] = the_command\n",
    "rollout = [state.pipeline_state]\n",
    "\n",
    "# Run trajectory\n",
    "n_steps = 500\n",
    "render_every = 2\n",
    "\n",
    "print(f\"Running {n_steps} steps with command: x={x_vel}, y={y_vel}, ang={ang_vel}\")\n",
    "for i in range(n_steps):\n",
    "  act_rng, rng = jax.random.split(rng)\n",
    "  ctrl, _ = jit_inference_fn(state.obs, act_rng)\n",
    "  state = jit_step(state, ctrl)\n",
    "  rollout.append(state.pipeline_state)\n",
    "\n",
    "print(f\"‚úÖ Simulation completed: {len(rollout)} frames\")\n",
    "\n",
    "# Render with MuJoCo renderer\n",
    "media.show_video(\n",
    "    eval_env.render(rollout[::render_every], camera='track'),\n",
    "    fps=1.0 / eval_env.dt / render_every)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dc22d6",
   "metadata": {},
   "source": [
    "## 12. Save Video File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f695d79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Alternative: Render with Brax HTML Renderer\n",
    "\n",
    "# This provides an interactive 3D visualization\n",
    "HTML(html.render(\n",
    "    eval_env.sys.tree_replace({'opt.timestep': eval_env.dt}),\n",
    "    rollout\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc29b74",
   "metadata": {},
   "source": [
    "## 13. Download All Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7bde11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Package and Download All Results\n",
    "\n",
    "# Create zip file with all outputs\n",
    "!zip -r /tmp/barkour_training_results.zip {ckpt_path} {model_path}\n",
    "\n",
    "print(\"\\nüì¶ All results packaged: /tmp/barkour_training_results.zip\")\n",
    "print(\"\\n‚úÖ Download from Files panel (üìÅ)\")\n",
    "print(\"\\nContains:\")\n",
    "print(f\"  - Final policy: {model_path}\")\n",
    "print(f\"  - Checkpoints: {ckpt_path}\")\n",
    "print(f\"\\nüí° Use these files with your local run_barkour_local.py script!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3b24f2",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What was trained:\n",
    "- **Robot**: Google Barkour VB quadruped from [MuJoCo Menagerie](https://github.com/google-deepmind/mujoco_menagerie/tree/main/google_barkour_vb)\n",
    "- **Task**: Joystick control (forward/sideways/turning locomotion)\n",
    "- **Algorithm**: PPO (Proximal Policy Optimization)\n",
    "- **Training steps**: 100M (100 million environment steps)\n",
    "- **Parallel environments**: 8192 (GPU-accelerated with MJX)\n",
    "- **Domain randomization**: Friction (0.6-1.4x) + Actuator parameters (¬±5 gain)\n",
    "\n",
    "### Files generated:\n",
    "1. **`/tmp/mjx_brax_quadruped_policy`** - Final trained policy (main file)\n",
    "2. **`/tmp/barkour_joystick/ckpts/`** - Intermediate checkpoints\n",
    "3. **Training progress plot** - Reward vs. steps visualization\n",
    "4. **Video demos** - Rendered robot locomotion\n",
    "\n",
    "### Joystick Commands:\n",
    "- **`x_vel`**: Forward/backward speed (-0.6 to 1.5 m/s)\n",
    "- **`y_vel`**: Sideways speed (-0.8 to 0.8 m/s)\n",
    "- **`ang_vel`**: Turning speed (-0.7 to 0.7 rad/s)\n",
    "\n",
    "### Next steps:\n",
    "1. **Download** the trained policy files from the Files panel (üìÅ)\n",
    "2. **Transfer** to your local machine: `c:\\users\\hatem\\Desktop\\MuJoCo\\training_checkpoints\\`\n",
    "3. **Run locally** using your `run_barkour_local.py` script\n",
    "4. **Experiment** with different commands and observe locomotion behaviors\n",
    "\n",
    "### Based on:\n",
    "This notebook is directly adapted from the [official MuJoCo MJX Tutorial](https://colab.research.google.com/github/google-deepmind/mujoco/blob/main/mjx/tutorial.ipynb) which demonstrates training quadruped policies with Brax and MJX."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
