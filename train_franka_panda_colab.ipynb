{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cb7ca7f",
   "metadata": {},
   "source": [
    "# Franka Emika Panda Manipulation Training on Google Colab\n",
    "\n",
    "This notebook trains a PPO policy for the Franka Emika Panda robotic arm using Brax and MuJoCo MJX.\n",
    "\n",
    "**Based on the official [MuJoCo MJX Tutorial](https://colab.research.google.com/github/google-deepmind/mujoco/blob/main/mjx/tutorial.ipynb)**\n",
    "\n",
    "## Setup Instructions:\n",
    "1. **Runtime → Change runtime type → T4 GPU** (or A100 for faster training)\n",
    "2. Run all cells in order\n",
    "3. Training takes ~5-10 minutes on A100 GPU, ~20-30 minutes on T4 GPU\n",
    "4. Download trained policy from Files panel\n",
    "\n",
    "## Features:\n",
    "- ✅ Robotic arm reaching task\n",
    "- ✅ 7-DOF arm + 2-finger gripper (8 actuators)\n",
    "- ✅ Domain randomization (friction + actuator parameters)\n",
    "- ✅ PPO training with Brax\n",
    "- ✅ Target reaching with end-effector\n",
    "- ✅ Policy visualization and video export\n",
    "\n",
    "## Task Description:\n",
    "The Panda arm learns to reach a randomly placed target sphere with its end-effector (gripper)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc086b13",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c91949e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mujoco mujoco_mjx brax\n",
    "!pip install flax optax orbax-checkpoint\n",
    "!pip install mediapy\n",
    "!command -v ffmpeg >/dev/null || (apt update && apt install -y ffmpeg)\n",
    "\n",
    "print(\"\\n✅ All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80c3eee",
   "metadata": {},
   "source": [
    "## 2. Verify GPU and Configure Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558d73bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Check GPU and Configure Environment\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Check GPU\n",
    "if subprocess.run('nvidia-smi').returncode:\n",
    "  raise RuntimeError(\n",
    "      'Cannot communicate with GPU. '\n",
    "      'Make sure you are using a GPU Colab runtime. '\n",
    "      'Go to the Runtime menu and select Choose runtime type.')\n",
    "\n",
    "# Add Nvidia EGL driver config for rendering\n",
    "NVIDIA_ICD_CONFIG_PATH = '/usr/share/glvnd/egl_vendor.d/10_nvidia.json'\n",
    "if not os.path.exists(NVIDIA_ICD_CONFIG_PATH):\n",
    "  with open(NVIDIA_ICD_CONFIG_PATH, 'w') as f:\n",
    "    f.write(\"\"\"{{\n",
    "    \"file_format_version\" : \"1.0.0\",\n",
    "    \"ICD\" : {{\n",
    "        \"library_path\" : \"libEGL_nvidia.so.0\"\n",
    "    }}\n",
    "}}\n",
    "\"\"\")\n",
    "\n",
    "# Configure MuJoCo to use EGL rendering (GPU)\n",
    "os.environ['MUJOCO_GL'] = 'egl'\n",
    "\n",
    "# XLA optimization for better performance\n",
    "xla_flags = os.environ.get('XLA_FLAGS', '')\n",
    "xla_flags += ' --xla_gpu_triton_gemm_any=True'\n",
    "os.environ['XLA_FLAGS'] = xla_flags\n",
    "\n",
    "# Test MuJoCo installation\n",
    "import mujoco\n",
    "mujoco.MjModel.from_xml_string('<mujoco/>')\n",
    "\n",
    "import jax\n",
    "print(f\"✅ GPU detected: {jax.devices()}\")\n",
    "print(f\"✅ MuJoCo configured with EGL rendering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0160b2",
   "metadata": {},
   "source": [
    "## 3. Download MuJoCo Menagerie Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8865b8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/google-deepmind/mujoco_menagerie.git\n",
    "print(\"\\n✅ MuJoCo Menagerie downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57436e2",
   "metadata": {},
   "source": [
    "## 4. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7a7e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from typing import Any, Dict, List, Sequence, Tuple\n",
    "from datetime import datetime\n",
    "from etils import epath\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mediapy as media\n",
    "from IPython.display import HTML\n",
    "\n",
    "from ml_collections import config_dict\n",
    "from flax.training import orbax_utils\n",
    "from orbax import checkpoint as ocp\n",
    "\n",
    "import mujoco\n",
    "from mujoco import mjx\n",
    "\n",
    "from brax import base\n",
    "from brax import envs\n",
    "from brax import math\n",
    "from brax.base import Motion, Transform\n",
    "from brax.envs.base import PipelineEnv, State\n",
    "from brax.training.agents.ppo import train as ppo\n",
    "from brax.training.agents.ppo import networks as ppo_networks\n",
    "from brax.io import html, mjcf, model\n",
    "\n",
    "# More legible numpy printing\n",
    "np.set_printoptions(precision=3, suppress=True, linewidth=100)\n",
    "\n",
    "print(\"✅ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa7af19",
   "metadata": {},
   "source": [
    "## 5. Define Franka Panda Reaching Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3705f98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Franka Panda Reaching Environment Definition\n",
    "\n",
    "PANDA_ROOT_PATH = epath.Path('mujoco_menagerie/franka_emika_panda')\n",
    "\n",
    "\n",
    "class FrankaPandaReachEnv(PipelineEnv):\n",
    "  \"\"\"Environment for training Franka Panda to reach a target with its end-effector.\"\"\"\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      distance_reward_weight=10.0,\n",
    "      ctrl_cost_weight=0.01,\n",
    "      reaching_reward=100.0,\n",
    "      reaching_threshold=0.05,  # 5cm distance\n",
    "      target_range=0.3,  # Target spawns within 30cm sphere\n",
    "      reset_noise_scale=0.01,\n",
    "      **kwargs,\n",
    "  ):\n",
    "    # Load MJX version of Panda (optimized for MJX)\n",
    "    path = PANDA_ROOT_PATH / 'mjx_scene.xml'\n",
    "    mj_model = mujoco.MjModel.from_xml_path(path.as_posix())\n",
    "    \n",
    "    # Add target sphere to the scene\n",
    "    xml = f\"\"\"\n",
    "    <mujoco model=\"panda_reach\">\n",
    "      <include file=\"{path.as_posix()}\"/>\n",
    "      \n",
    "      <worldbody>\n",
    "        <body name=\"target\" pos=\"0.5 0 0.5\">\n",
    "          <geom name=\"target\" type=\"sphere\" size=\"0.03\" rgba=\"1 0 0 0.5\" \n",
    "                contype=\"0\" conaffinity=\"0\"/>\n",
    "          <site name=\"target_site\" size=\"0.01\"/>\n",
    "        </body>\n",
    "      </worldbody>\n",
    "    </mujoco>\n",
    "    \"\"\"\n",
    "    mj_model = mujoco.MjModel.from_xml_string(xml)\n",
    "    \n",
    "    # Optimize solver for manipulation\n",
    "    mj_model.opt.solver = mujoco.mjtSolver.mjSOL_CG\n",
    "    mj_model.opt.iterations = 6\n",
    "    mj_model.opt.ls_iterations = 6\n",
    "\n",
    "    sys = mjcf.load_model(mj_model)\n",
    "\n",
    "    physics_steps_per_control_step = 5\n",
    "    kwargs['n_frames'] = kwargs.get(\n",
    "        'n_frames', physics_steps_per_control_step)\n",
    "    kwargs['backend'] = 'mjx'\n",
    "\n",
    "    super().__init__(sys, **kwargs)\n",
    "\n",
    "    self._distance_reward_weight = distance_reward_weight\n",
    "    self._ctrl_cost_weight = ctrl_cost_weight\n",
    "    self._reaching_reward = reaching_reward\n",
    "    self._reaching_threshold = reaching_threshold\n",
    "    self._target_range = target_range\n",
    "    self._reset_noise_scale = reset_noise_scale\n",
    "    \n",
    "    # Find important body/site indices\n",
    "    self._target_body_id = mujoco.mj_name2id(\n",
    "        mj_model, mujoco.mjtObj.mjOBJ_BODY, 'target')\n",
    "    self._ee_site_id = mujoco.mj_name2id(\n",
    "        mj_model, mujoco.mjtObj.mjOBJ_SITE, 'attachment_site')  # End-effector site\n",
    "\n",
    "  def reset(self, rng: jp.ndarray) -> State:\n",
    "    \"\"\"Resets the environment to an initial state.\"\"\"\n",
    "    rng, rng1, rng2, rng3 = jax.random.split(rng, 4)\n",
    "\n",
    "    # Reset robot with small noise\n",
    "    low, hi = -self._reset_noise_scale, self._reset_noise_scale\n",
    "    qpos = self.sys.qpos0 + jax.random.uniform(\n",
    "        rng1, (self.sys.nq,), minval=low, maxval=hi\n",
    "    )\n",
    "    qvel = jax.random.uniform(\n",
    "        rng2, (self.sys.nv,), minval=low, maxval=hi\n",
    "    )\n",
    "    \n",
    "    # Randomize target position in workspace\n",
    "    target_pos = jax.random.uniform(\n",
    "        rng3, (3,), \n",
    "        minval=jp.array([0.3, -0.3, 0.2]),\n",
    "        maxval=jp.array([0.7, 0.3, 0.6])\n",
    "    )\n",
    "    # Set target body position (first 3 elements after robot qpos)\n",
    "    # Panda has 9 DOF, target has 7 (3 pos + 4 quat)\n",
    "    qpos = qpos.at[9:12].set(target_pos)  # Set target xyz position\n",
    "\n",
    "    data = self.pipeline_init(qpos, qvel)\n",
    "\n",
    "    obs = self._get_obs(data)\n",
    "    reward, done, zero = jp.zeros(3)\n",
    "    metrics = {\n",
    "        'distance_reward': zero,\n",
    "        'ctrl_cost': zero,\n",
    "        'reaching_reward': zero,\n",
    "        'distance_to_target': zero,\n",
    "        'success': zero,\n",
    "    }\n",
    "    return State(data, obs, reward, done, metrics)\n",
    "\n",
    "  def step(self, state: State, action: jp.ndarray) -> State:\n",
    "    \"\"\"Runs one timestep of the environment's dynamics.\"\"\"\n",
    "    data0 = state.pipeline_state\n",
    "    data = self.pipeline_step(data0, action)\n",
    "\n",
    "    # Get end-effector position (attachment_site on hand)\n",
    "    ee_pos = data.site_xpos[self._ee_site_id]\n",
    "    \n",
    "    # Get target position\n",
    "    target_pos = data.xpos[self._target_body_id]\n",
    "    \n",
    "    # Calculate distance to target\n",
    "    distance = jp.linalg.norm(ee_pos - target_pos)\n",
    "    \n",
    "    # Distance reward (negative distance, so closer = higher reward)\n",
    "    distance_reward = -self._distance_reward_weight * distance\n",
    "    \n",
    "    # Reaching bonus (if within threshold)\n",
    "    reaching_reward = jp.where(\n",
    "        distance < self._reaching_threshold,\n",
    "        self._reaching_reward,\n",
    "        0.0\n",
    "    )\n",
    "    \n",
    "    # Control cost (penalize large actions)\n",
    "    ctrl_cost = self._ctrl_cost_weight * jp.sum(jp.square(action))\n",
    "    \n",
    "    # Success indicator\n",
    "    success = jp.where(distance < self._reaching_threshold, 1.0, 0.0)\n",
    "\n",
    "    obs = self._get_obs(data)\n",
    "    reward = distance_reward + reaching_reward - ctrl_cost\n",
    "    done = 0.0  # No terminal condition for reaching task\n",
    "    \n",
    "    state.metrics.update(\n",
    "        distance_reward=distance_reward,\n",
    "        ctrl_cost=ctrl_cost,\n",
    "        reaching_reward=reaching_reward,\n",
    "        distance_to_target=distance,\n",
    "        success=success,\n",
    "    )\n",
    "\n",
    "    return state.replace(\n",
    "        pipeline_state=data, obs=obs, reward=reward, done=done\n",
    "    )\n",
    "\n",
    "  def _get_obs(self, data: mjx.Data) -> jp.ndarray:\n",
    "    \"\"\"Observes robot joint positions, velocities, and relative target position.\"\"\"\n",
    "    # Robot state (9 DOF: 7 arm + 2 gripper)\n",
    "    robot_qpos = data.qpos[:9]\n",
    "    robot_qvel = data.qvel[:9]\n",
    "    \n",
    "    # End-effector position\n",
    "    ee_pos = data.site_xpos[self._ee_site_id]\n",
    "    \n",
    "    # Target position\n",
    "    target_pos = data.xpos[self._target_body_id]\n",
    "    \n",
    "    # Relative target position (goal - current)\n",
    "    relative_target = target_pos - ee_pos\n",
    "    \n",
    "    return jp.concatenate([\n",
    "        robot_qpos,\n",
    "        robot_qvel,\n",
    "        ee_pos,\n",
    "        target_pos,\n",
    "        relative_target,\n",
    "    ])\n",
    "\n",
    "\n",
    "envs.register_environment('franka_panda_reach', FrankaPandaReachEnv)\n",
    "print(\"✅ FrankaPandaReachEnv registered!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1c468f",
   "metadata": {},
   "source": [
    "## 6. Domain Randomization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84998a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Domain Randomization Function\n",
    "\n",
    "def domain_randomize(sys, rng):\n",
    "  \"\"\"Randomizes the mjx.Model for better sim-to-real transfer.\"\"\"\n",
    "  @jax.vmap\n",
    "  def rand(rng):\n",
    "    _, key = jax.random.split(rng, 2)\n",
    "    # friction randomization\n",
    "    friction = jax.random.uniform(key, (1,), minval=0.6, maxval=1.4)\n",
    "    friction = sys.geom_friction.at[:, 0].set(friction)\n",
    "    # actuator randomization (for arm actuators)\n",
    "    _, key = jax.random.split(key, 2)\n",
    "    gain_range = (-5, 5)\n",
    "    param = jax.random.uniform(\n",
    "        key, (1,), minval=gain_range[0], maxval=gain_range[1]\n",
    "    ) + sys.actuator_gainprm[:, 0]\n",
    "    gain = sys.actuator_gainprm.at[:, 0].set(param)\n",
    "    bias = sys.actuator_biasprm.at[:, 1].set(-param)\n",
    "    return friction, gain, bias\n",
    "\n",
    "  friction, gain, bias = rand(rng)\n",
    "\n",
    "  in_axes = jax.tree_util.tree_map(lambda x: None, sys)\n",
    "  in_axes = in_axes.tree_replace({\n",
    "      'geom_friction': 0,\n",
    "      'actuator_gainprm': 0,\n",
    "      'actuator_biasprm': 0,\n",
    "  })\n",
    "\n",
    "  sys = sys.tree_replace({\n",
    "      'geom_friction': friction,\n",
    "      'actuator_gainprm': gain,\n",
    "      'actuator_biasprm': bias,\n",
    "  })\n",
    "\n",
    "  return sys, in_axes\n",
    "\n",
    "\n",
    "print(\"✅ Domain randomization function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f437de",
   "metadata": {},
   "source": [
    "## 7. Create Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18c9ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env_name = 'franka_panda_reach'\n",
    "env = envs.get_environment(env_name)\n",
    "\n",
    "print(f\"✅ Environment created!\")\n",
    "print(f\"  - Observation size: {env.observation_size}\")\n",
    "print(f\"  - Action size: {env.action_size}\")\n",
    "print(f\"  - DOF: {env.sys.nq} position, {env.sys.nv} velocity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407664f9",
   "metadata": {},
   "source": [
    "## 8. Visualize Untrained Policy (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a710b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Quick Test: Visualize Random Actions\n",
    "\n",
    "jit_reset = jax.jit(env.reset)\n",
    "jit_step = jax.jit(env.step)\n",
    "\n",
    "# Initialize\n",
    "state = jit_reset(jax.random.PRNGKey(0))\n",
    "rollout = [state.pipeline_state]\n",
    "\n",
    "# Grab a short trajectory with random actions\n",
    "for i in range(100):\n",
    "  ctrl = 0.1 * jax.random.normal(jax.random.PRNGKey(i), (env.sys.nu,))\n",
    "  state = jit_step(state, ctrl)\n",
    "  rollout.append(state.pipeline_state)\n",
    "\n",
    "media.show_video(env.render(rollout, camera='side'), fps=1.0 / env.dt)\n",
    "print(\"\\n⚠️ This is WITHOUT training - random arm movements!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d681f83",
   "metadata": {},
   "source": [
    "## 9. Train Policy with Progress Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169f5bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Train Policy with Progress Tracking\n",
    "\n",
    "# Training configuration (manipulation tasks learn faster than locomotion)\n",
    "NUM_TIMESTEPS = 10_000_000  # 10M steps for reaching task\n",
    "NUM_EVALS = 5\n",
    "EPISODE_LENGTH = 500  # Shorter episodes for reaching\n",
    "NUM_ENVS = 4096  # Good for manipulation\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "# Progress tracking\n",
    "x_data = []\n",
    "y_data = []\n",
    "ydataerr = []\n",
    "times = [datetime.now()]\n",
    "max_y, min_y = 5000, -1000\n",
    "\n",
    "def progress(num_steps, metrics):\n",
    "  \"\"\"Callback to track and display training progress.\"\"\"\n",
    "  times.append(datetime.now())\n",
    "  x_data.append(num_steps)\n",
    "  y_data.append(metrics['eval/episode_reward'])\n",
    "  ydataerr.append(metrics['eval/episode_reward_std'])\n",
    "\n",
    "  plt.xlim([0, NUM_TIMESTEPS * 1.25])\n",
    "  plt.ylim([min_y, max_y])\n",
    "  plt.xlabel('# environment steps')\n",
    "  plt.ylabel('reward per episode')\n",
    "  plt.title(f'Franka Panda Reaching | Reward: {y_data[-1]:.3f}')\n",
    "  plt.errorbar(x_data, y_data, yerr=ydataerr)\n",
    "  plt.grid(True, alpha=0.3)\n",
    "  plt.show()\n",
    "\n",
    "  print(f\"Steps: {num_steps:,} | Reward: {metrics['eval/episode_reward']:.2f}\")\n",
    "\n",
    "# Setup checkpoint saving\n",
    "ckpt_path = epath.Path('/tmp/franka_panda_reach/ckpts')\n",
    "ckpt_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def policy_params_fn(current_step, make_policy, params):\n",
    "  \"\"\"Save checkpoints during training.\"\"\"\n",
    "  orbax_checkpointer = ocp.PyTreeCheckpointer()\n",
    "  save_args = orbax_utils.save_args_from_target(params)\n",
    "  path = ckpt_path / f'{current_step}'\n",
    "  orbax_checkpointer.save(path, params, force=True, save_args=save_args)\n",
    "  print(f\"💾 Checkpoint saved: {path}\")\n",
    "\n",
    "# Configure PPO training (standard network for manipulation)\n",
    "make_networks_factory = functools.partial(\n",
    "    ppo_networks.make_ppo_networks,\n",
    "    policy_hidden_layer_sizes=(128, 128, 128))  # Standard size for manipulation\n",
    "\n",
    "train_fn = functools.partial(\n",
    "    ppo.train,\n",
    "    num_timesteps=NUM_TIMESTEPS,\n",
    "    num_evals=NUM_EVALS,\n",
    "    reward_scaling=0.1,\n",
    "    episode_length=EPISODE_LENGTH,\n",
    "    normalize_observations=True,\n",
    "    action_repeat=1,\n",
    "    unroll_length=10,\n",
    "    num_minibatches=24,\n",
    "    num_updates_per_batch=8,\n",
    "    discounting=0.97,\n",
    "    learning_rate=3e-4,\n",
    "    entropy_cost=1e-3,\n",
    "    num_envs=NUM_ENVS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    network_factory=make_networks_factory,\n",
    "    randomization_fn=domain_randomize,\n",
    "    policy_params_fn=policy_params_fn,\n",
    "    seed=0,\n",
    "    progress_fn=progress\n",
    ")\n",
    "\n",
    "# Reset environment and start training\n",
    "env = envs.get_environment(env_name)\n",
    "eval_env = envs.get_environment(env_name)\n",
    "\n",
    "print(f\"\\n🚀 Starting training...\")\n",
    "print(f\"  - Total steps: {NUM_TIMESTEPS:,}\")\n",
    "print(f\"  - Parallel envs: {NUM_ENVS}\")\n",
    "print(f\"  - Estimated time: ~5-10 min on A100, ~20-30 min on T4 GPU\")\n",
    "print(f\"  - Task: Reach randomly placed target with end-effector\\n\")\n",
    "\n",
    "make_inference_fn, params, _ = train_fn(\n",
    "    environment=env,\n",
    "    eval_env=eval_env\n",
    ")\n",
    "\n",
    "print(f'\\n🎉 Training completed!')\n",
    "print(f'  - Time to JIT: {times[1] - times[0]}')\n",
    "print(f'  - Time to train: {times[-1] - times[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33e4b56",
   "metadata": {},
   "source": [
    "## 10. Plot Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73fa598",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x_data, y_data, 'b-', linewidth=2)\n",
    "plt.fill_between(x_data, \n",
    "                 [y - err for y, err in zip(y_data, ydataerr)],\n",
    "                 [y + err for y, err in zip(y_data, ydataerr)],\n",
    "                 alpha=0.3)\n",
    "plt.xlabel('Training Steps', fontsize=12)\n",
    "plt.ylabel('Episode Reward', fontsize=12)\n",
    "plt.title('Franka Panda Reaching Training Progress', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc2d1a9",
   "metadata": {},
   "source": [
    "## 11. Save Trained Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d5ee9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and reload params\n",
    "model_path = '/tmp/mjx_brax_franka_panda_policy'\n",
    "model.save_params(model_path, params)\n",
    "\n",
    "print(f\"✅ Final policy saved: {model_path}\")\n",
    "print(f\"\\n📦 To download:\")\n",
    "print(f\"1. Click Files icon (📁) in left sidebar\")\n",
    "print(f\"2. Navigate to {ckpt_path} or {model_path}\")\n",
    "print(f\"3. Right-click → Download\")\n",
    "\n",
    "# Reload params and create inference function\n",
    "params = model.load_params(model_path)\n",
    "inference_fn = make_inference_fn(params)\n",
    "jit_inference_fn = jax.jit(inference_fn)\n",
    "\n",
    "print(f\"\\n✅ Policy reloaded and ready for visualization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8465df78",
   "metadata": {},
   "source": [
    "## 12. Visualize Trained Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1e29ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Visualize Trained Policy\n",
    "\n",
    "# Create evaluation environment\n",
    "eval_env = envs.get_environment(env_name)\n",
    "jit_reset = jax.jit(eval_env.reset)\n",
    "jit_step = jax.jit(eval_env.step)\n",
    "\n",
    "# Initialize\n",
    "rng = jax.random.PRNGKey(42)\n",
    "state = jit_reset(rng)\n",
    "rollout = [state.pipeline_state]\n",
    "\n",
    "# Run trajectory\n",
    "n_steps = 250\n",
    "render_every = 2\n",
    "\n",
    "print(f\"Running {n_steps} steps with trained policy...\")\n",
    "for i in range(n_steps):\n",
    "  act_rng, rng = jax.random.split(rng)\n",
    "  ctrl, _ = jit_inference_fn(state.obs, act_rng)\n",
    "  state = jit_step(state, ctrl)\n",
    "  rollout.append(state.pipeline_state)\n",
    "  \n",
    "  # Print success when target is reached\n",
    "  if state.metrics['success'] > 0.5 and i % 50 == 0:\n",
    "    print(f\"  ✅ Target reached at step {i}! Distance: {state.metrics['distance_to_target']:.3f}m\")\n",
    "\n",
    "print(f\"✅ Simulation completed: {len(rollout)} frames\")\n",
    "\n",
    "# Render with MuJoCo renderer\n",
    "media.show_video(\n",
    "    eval_env.render(rollout[::render_every], camera='side'),\n",
    "    fps=1.0 / eval_env.dt / render_every)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0383659d",
   "metadata": {},
   "source": [
    "## 13. Alternative: Render with Brax HTML Renderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda20462",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Alternative: Render with Brax HTML Renderer\n",
    "\n",
    "# This provides an interactive 3D visualization\n",
    "HTML(html.render(\n",
    "    eval_env.sys.tree_replace({'opt.timestep': eval_env.dt}),\n",
    "    rollout\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200e2171",
   "metadata": {},
   "source": [
    "## 14. Test Multiple Random Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb9dcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Test Multiple Random Targets\n",
    "\n",
    "# Test success rate across multiple random targets\n",
    "num_tests = 10\n",
    "success_count = 0\n",
    "distances = []\n",
    "\n",
    "print(f\"Testing policy on {num_tests} random targets...\\n\")\n",
    "\n",
    "for test_id in range(num_tests):\n",
    "  rng = jax.random.PRNGKey(test_id + 100)\n",
    "  state = jit_reset(rng)\n",
    "  \n",
    "  # Run episode\n",
    "  for i in range(EPISODE_LENGTH):\n",
    "    act_rng, rng = jax.random.split(rng)\n",
    "    ctrl, _ = jit_inference_fn(state.obs, act_rng)\n",
    "    state = jit_step(state, ctrl)\n",
    "  \n",
    "  final_distance = float(state.metrics['distance_to_target'])\n",
    "  success = float(state.metrics['success']) > 0.5\n",
    "  \n",
    "  distances.append(final_distance)\n",
    "  if success:\n",
    "    success_count += 1\n",
    "  \n",
    "  status = \"✅ SUCCESS\" if success else \"❌ FAILED\"\n",
    "  print(f\"Test {test_id + 1}: {status} | Final distance: {final_distance:.4f}m\")\n",
    "\n",
    "print(f\"\\n📊 Results:\")\n",
    "print(f\"  - Success rate: {success_count}/{num_tests} ({100*success_count/num_tests:.1f}%)\")\n",
    "print(f\"  - Average distance: {np.mean(distances):.4f}m\")\n",
    "print(f\"  - Min distance: {np.min(distances):.4f}m\")\n",
    "print(f\"  - Max distance: {np.max(distances):.4f}m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859d8afc",
   "metadata": {},
   "source": [
    "## 15. Package and Download All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea190841",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Package and Download All Results\n",
    "\n",
    "# Create zip file with all outputs\n",
    "!zip -r /tmp/franka_panda_training_results.zip {ckpt_path} {model_path}\n",
    "\n",
    "print(\"\\n📦 All results packaged: /tmp/franka_panda_training_results.zip\")\n",
    "print(\"\\n✅ Download from Files panel (📁)\")\n",
    "print(\"\\nContains:\")\n",
    "print(f\"  - Final policy: {model_path}\")\n",
    "print(f\"  - Checkpoints: {ckpt_path}\")\n",
    "print(f\"\\n💡 Use these files with a local inference script!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bd33ef",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What was trained:\n",
    "- **Robot**: Franka Emika Panda from [MuJoCo Menagerie](https://github.com/google-deepmind/mujoco_menagerie/tree/main/franka_emika_panda)\n",
    "- **Task**: Reaching task - move end-effector to randomly placed target sphere\n",
    "- **Algorithm**: PPO (Proximal Policy Optimization)\n",
    "- **Training steps**: 10M (10 million environment steps)\n",
    "- **Parallel environments**: 4096 (GPU-accelerated with MJX)\n",
    "- **Domain randomization**: Friction (0.6-1.4x) + Actuator parameters (±5 gain)\n",
    "\n",
    "### Robot Specifications:\n",
    "- **Type**: 7-DOF robotic manipulator + 2-finger parallel gripper\n",
    "- **Total DOF**: 8 actuated joints (7 arm + 1 gripper)\n",
    "- **Workspace**: ~0.85m reach radius\n",
    "- **Observation space**: 27 dimensions\n",
    "  - Joint positions (9)\n",
    "  - Joint velocities (9)\n",
    "  - End-effector position (3)\n",
    "  - Target position (3)\n",
    "  - Relative target vector (3)\n",
    "- **Action space**: 8 dimensions (position control)\n",
    "\n",
    "### Reward Function:\n",
    "- **Distance reward**: -10 × distance (closer = higher reward)\n",
    "- **Reaching bonus**: +100 when within 5cm of target\n",
    "- **Control cost**: -0.01 × sum of squared actions\n",
    "\n",
    "### Files generated:\n",
    "1. **`/tmp/mjx_brax_franka_panda_policy`** - Final trained policy\n",
    "2. **`/tmp/franka_panda_reach/ckpts/`** - Intermediate checkpoints\n",
    "3. **Training progress plot** - Reward vs. steps visualization\n",
    "4. **Video demos** - Rendered reaching behavior\n",
    "\n",
    "### Training Notes:\n",
    "- **Manipulation tasks** learn faster than locomotion (10M vs 100M steps)\n",
    "- Reaching is a **standard benchmark** for robotic manipulation\n",
    "- Target position is **randomized** each episode for generalization\n",
    "- The policy learns to reach from any starting configuration\n",
    "- Domain randomization helps with sim-to-real transfer\n",
    "\n",
    "### Expected Behavior:\n",
    "With 10M steps training:\n",
    "- ✅ Consistent reaching to random targets\n",
    "- ✅ Success rate: 80-95%\n",
    "- ✅ Smooth, efficient trajectories\n",
    "- ✅ Generalizes to unseen target positions\n",
    "\n",
    "For even better performance, train for 20M+ steps (~10-15 min on A100).\n",
    "\n",
    "### Differences from Locomotion:\n",
    "| Aspect | Locomotion (Barkour/G1) | Manipulation (Panda) |\n",
    "|--------|-------------------------|----------------------|\n",
    "| Task | Walking/running | Reaching target |\n",
    "| Episode length | 1000 steps | 500 steps |\n",
    "| Training steps | 20M-100M | 10M-20M |\n",
    "| Success metric | Distance traveled | Target reached |\n",
    "| Observation | Body pose, velocities | Joint state, target position |\n",
    "| Termination | Falling | None (continuous) |\n",
    "\n",
    "### Next Steps:\n",
    "- **Object manipulation**: Add objects to grasp and move\n",
    "- **Multi-target**: Reach sequence of targets\n",
    "- **Obstacle avoidance**: Add obstacles in workspace\n",
    "- **Contact-rich tasks**: Push, slide, insert objects\n",
    "\n",
    "### Based on:\n",
    "- [Official MuJoCo MJX Tutorial](https://colab.research.google.com/github/google-deepmind/mujoco/blob/main/mjx/tutorial.ipynb)\n",
    "- [Franka Emika Panda](https://www.franka.de/) official model\n",
    "- Standard robotic reaching task from RL literature"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
